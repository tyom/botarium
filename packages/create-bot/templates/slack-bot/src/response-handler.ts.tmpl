/**
 * Response Handler
 *
 * This is the main file to customize bot responses.
 */
{{#if isAi}}
import { getModel, settings } from './settings'
import { streamAIResponse } from './ai/stream'
{{/if}}
{{#if isDb}}
import { buildMemoryContext, extractAndSaveMemory } from './memory'
import { memoryLogger } from './utils/logger'
{{/if}}

export interface ThreadContext {
  channelId: string
  threadTs: string
  userId: string
  teamId: string
  history: Array<{ role: 'user' | 'assistant'; content: string }>
}

export interface SuggestedPrompt {
  title: string
  message: string
}

export interface ResponseHandler {
  generateResponse(
    message: string,
    context: ThreadContext
  ): AsyncIterable<string>
  systemPrompt?: string
  suggestedPrompts?: SuggestedPrompt[]
}
{{#if isDb}}

async function buildEnhancedSystemPrompt(basePrompt: string, context: ThreadContext): Promise<string> {
  const memoryContext = await buildMemoryContext({
    userId: context.userId,
    teamId: context.teamId,
  })
  return [basePrompt, memoryContext].filter(Boolean).join('\n\n')
}
{{/if}}

export const responseHandler: ResponseHandler = {
  systemPrompt: '{{#if isAi}}You are a helpful assistant. Be concise and friendly.{{else}}You are a helpful assistant.{{/if}}',

  suggestedPrompts: [
    { title: 'Say hello', message: 'Hello!' },
    { title: 'Get help', message: 'What can you help me with?' },
  ],

  async *generateResponse(message, context) {
    // Handle ping command
    if (message.toLowerCase() === 'ping') {
      yield 'pong'
      return
    }
{{#if isAi}}

    const model = getModel()

    // Apply history limit (0 = no history, capped at 100)
    const historyLimit = Math.min(settings.CONTEXT_HISTORY_LIMIT ?? 20, 100)
    const limitedHistory = historyLimit > 0
      ? context.history.slice(-historyLimit)
      : []

    // Build messages from history
    const messages = limitedHistory
      .filter(m => m.content.trim())
      .map(m => ({
        role: m.role as 'user' | 'assistant',
        content: m.content,
      }))

    // Add current message
    messages.push({ role: 'user', content: message })
{{#if isDb}}

    // Build enhanced system prompt with memory
    const systemPrompt = await buildEnhancedSystemPrompt(this.systemPrompt ?? '', context)
{{/if}}

    // Stream response from AI
    yield* streamAIResponse({
      model,
      system: {{#if isDb}}systemPrompt{{else}}this.systemPrompt{{/if}},
      messages,
    })
{{#if isDb}}

    // Extract and save any memories from the user's message (fire and forget)
    extractAndSaveMemory({
      model,
      message,
      history: context.history,
      userId: context.userId,
      teamId: context.teamId,
      sourceChannel: context.channelId,
      sourceThread: context.threadTs,
    }).catch((err) => {
      memoryLogger.error({ err }, 'Failed to extract and save memory')
    })
{{/if}}
{{else}}

    // Echo response - replace with your logic
    yield `You said: ${message}`
{{/if}}
  },
}
